{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train size: 42395 Validation size: 10599\n",
      "Class Distribution (Train):\n",
      "Priority\n",
      "2.0    24923\n",
      "1.0     7530\n",
      "0.0     7154\n",
      "3.0     1830\n",
      "4.0      958\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv('/home/da23c014/ISEC/dataset/combined_data.csv')\n",
    "\n",
    "# Basic preprocessing\n",
    "data_df['cleaned_title'] = data_df['cleaned_title'].fillna('')\n",
    "data_df['cleaned_description'] = data_df['cleaned_description'].fillna('')\n",
    "data_df['Component'] = data_df['Component'].astype(str)\n",
    "data_df['Status'] = data_df['Status'].astype(str)\n",
    "data_df['Resolution'] = data_df['Resolution'].astype(str)\n",
    "\n",
    "# Separate train/valid based on Usage\n",
    "train_df = data_df[data_df['Usage'] == 'Train'].copy()\n",
    "valid_df = data_df[data_df['Usage'] == 'Valid'].copy()\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Validation size:\", len(valid_df))\n",
    "\n",
    "# Class distribution\n",
    "print(\"Class Distribution (Train):\")\n",
    "print(train_df['Priority'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)\n",
    "max_len = 512\n",
    "\n",
    "def combine_text(row):\n",
    "    # We focus on text: use title + [SEP] + description\n",
    "    return row['cleaned_title'] + \" [SEP] \" + row['cleaned_description']\n",
    "\n",
    "train_df['text_input'] = train_df.apply(combine_text, axis=1)\n",
    "valid_df['text_input'] = valid_df.apply(combine_text, axis=1)\n",
    "\n",
    "# Tokenize\n",
    "def encode_texts(texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "train_input_ids, train_attention_masks = encode_texts(train_df['text_input'].tolist())\n",
    "valid_input_ids, valid_attention_masks = encode_texts(valid_df['text_input'].tolist())\n",
    "\n",
    "# Labels\n",
    "train_labels = torch.tensor(train_df['Priority'].values, dtype=torch.long)\n",
    "valid_labels = torch.tensor(valid_df['Priority'].values, dtype=torch.long)\n",
    "\n",
    "# Categorical features\n",
    "components = list(data_df['Component'].unique())\n",
    "status_vals = list(data_df['Status'].unique())\n",
    "resolution_vals = list(data_df['Resolution'].unique())\n",
    "\n",
    "component2idx = {c: i for i, c in enumerate(components)}\n",
    "status2idx = {s: i for i, s in enumerate(status_vals)}\n",
    "resolution2idx = {r: i for i, r in enumerate(resolution_vals)}\n",
    "\n",
    "def encode_cat(df):\n",
    "    cat_component = df['Component'].map(component2idx).values\n",
    "    cat_status = df['Status'].map(status2idx).values\n",
    "    cat_resolution = df['Resolution'].map(resolution2idx).values\n",
    "    return torch.tensor(np.stack([cat_component, cat_status, cat_resolution], axis=1), dtype=torch.long)\n",
    "\n",
    "train_cat_feats = encode_cat(train_df)\n",
    "valid_cat_feats = encode_cat(valid_df)\n",
    "\n",
    "# Numeric features\n",
    "# Using word_count and description_word_count\n",
    "train_num = train_df[['word_count', 'description_word_count']].fillna(0).values.astype(np.float32)\n",
    "valid_num = valid_df[['word_count', 'description_word_count']].fillna(0).values.astype(np.float32)\n",
    "\n",
    "# Normalize numeric features based on train stats\n",
    "num_mean = train_num.mean(axis=0)\n",
    "num_std = train_num.std(axis=0) + 1e-8\n",
    "train_num = (train_num - num_mean) / num_std\n",
    "valid_num = (valid_num - num_mean) / num_std\n",
    "\n",
    "train_num_feats = torch.tensor(train_num, dtype=torch.float32)\n",
    "valid_num_feats = torch.tensor(valid_num, dtype=torch.float32)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_cat_feats, train_num_feats, train_labels)\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_attention_masks, valid_cat_feats, valid_num_feats, valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution (train) for weighting\n",
    "# Frequencies provided:\n",
    "# 2:31154, 1:9412, 0:8943, 3:2287, 4:1198\n",
    "freqs = {0:8943, 1:9412, 2:31154, 3:2287, 4:1198}\n",
    "# Let's do inverse frequency normalized:\n",
    "class_weights = []\n",
    "for c in range(5):\n",
    "    class_weights.append(1.0 / freqs[c])\n",
    "class_weights = np.array(class_weights)\n",
    "class_weights = class_weights / class_weights.sum()  # normalize\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "\n",
    "class MultiModalRoberta(nn.Module):\n",
    "    def __init__(self, \n",
    "                 roberta_model_name='roberta-large', \n",
    "                 num_labels=5, \n",
    "                 cat_vocab_sizes=[len(components), len(status_vals), len(resolution_vals)],\n",
    "                 cat_emb_dim=32,\n",
    "                 num_numeric=2, \n",
    "                 hidden_size=1024, # roberta-large hidden size\n",
    "                 dropout_prob=0.1):\n",
    "        super(MultiModalRoberta, self).__init__()\n",
    "        \n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        \n",
    "        # Embeddings for categorical features\n",
    "        self.cat_embeddings = nn.ModuleList([nn.Embedding(vocab_size, cat_emb_dim) for vocab_size in cat_vocab_sizes])\n",
    "        cat_total_dim = cat_emb_dim * len(cat_vocab_sizes)\n",
    "        \n",
    "        # A small MLP for numeric features\n",
    "        self.num_mlp = nn.Sequential(\n",
    "            nn.Linear(num_numeric, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Final classifier layer\n",
    "        # Input: CLS (1024) + cat_embs (cat_total_dim) + num_embs (32)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_size + cat_total_dim + 32, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, cat_feats, num_feats):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:,0,:]  # [batch, hidden_size]\n",
    "        \n",
    "        cat_embs = []\n",
    "        for i, emb_layer in enumerate(self.cat_embeddings):\n",
    "            cat_embs.append(emb_layer(cat_feats[:,i]))\n",
    "        cat_embs = torch.cat(cat_embs, dim=1) # [batch, cat_total_dim]\n",
    "        \n",
    "        num_embs = self.num_mlp(num_feats) # [batch, 32]\n",
    "        \n",
    "        combined = torch.cat([cls_emb, cat_embs, num_embs], dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.float()\n",
    "        targets = targets.long()\n",
    "        \n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    sampler=SequentialSampler(valid_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "model = MultiModalRoberta()\n",
    "model.to(device)\n",
    "\n",
    "# Define focal loss with class weights\n",
    "loss_fn = FocalLoss(gamma=2.0, alpha=class_weights, reduction='mean')\n",
    "\n",
    "# Stage 1 Hyperparams\n",
    "epochs_stage1 = 2\n",
    "learning_rate_stage1 = 2e-5\n",
    "\n",
    "# Only roberta + classifier trainable, freeze cat and num\n",
    "for param in model.cat_embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.num_mlp.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate_stage1, eps=1e-8)\n",
    "total_steps_stage1 = len(train_dataloader) * epochs_stage1\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps_stage1)\n",
    "\n",
    "# Early stopping\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            b_input_ids, b_input_mask, b_cat, b_num, b_labels = tuple(t.to(device) for t in batch)\n",
    "            logits = model(b_input_ids, b_input_mask, b_cat, b_num)\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            eval_loss += loss.item()\n",
    "            preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.append(b_labels.cpu().numpy())\n",
    "    avg_val_loss = eval_loss / len(dataloader)\n",
    "    preds = np.concatenate(preds)\n",
    "    true_labels = np.concatenate(true_labels)\n",
    "    val_f1 = f1_score(true_labels, preds, average='macro')\n",
    "    return avg_val_loss, val_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1485516/4161601121.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_stage1.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiModalRoberta(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cat_embeddings): ModuleList(\n",
       "    (0): Embedding(185, 32)\n",
       "    (1): Embedding(3, 32)\n",
       "    (2): Embedding(9, 32)\n",
       "  )\n",
       "  (num_mlp): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=1152, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_stage1.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** STAGE 1 TRAINING *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ff2ba23d374675821635a854be6458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stage 1 Epoch 1/2:   0%|          | 0/1928 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"***** STAGE 1 TRAINING *****\")\n",
    "\n",
    "for epoch_i in range(epochs_stage1):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Stage 1 Epoch {epoch_i+1}/{epochs_stage1}\")):\n",
    "        b_input_ids, b_input_mask, b_cat, b_num, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(b_input_ids, b_input_mask, b_cat, b_num)\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    val_loss, val_f1 = evaluate(model, validation_dataloader)\n",
    "    print(f\"Stage 1 Epoch {epoch_i+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping based on val loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_stage1.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping during Stage 1.\")\n",
    "            break\n",
    "            \n",
    "# Load best model from stage 1\n",
    "model.load_state_dict(torch.load('best_model_stage1.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze RoBERTa now\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze cat and numeric layers\n",
    "for param in model.cat_embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.num_mlp.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# You can also keep the classifier head trainable.\n",
    "# New optimizer & scheduler for stage 2\n",
    "epochs_stage2 = 4\n",
    "learning_rate_stage2 = 3e-4\n",
    "\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate_stage2, eps=1e-8)\n",
    "total_steps_stage2 = len(train_dataloader) * epochs_stage2\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps_stage2)\n",
    "\n",
    "best_val_loss_stage2 = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"***** STAGE 2 TRAINING *****\")\n",
    "for epoch_i in range(epochs_stage2):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Stage 2 Epoch {epoch_i+1}/{epochs_stage2}\")):\n",
    "        b_input_ids, b_input_mask, b_cat, b_num, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(b_input_ids, b_input_mask, b_cat, b_num)\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    val_loss, val_f1 = evaluate(model, validation_dataloader)\n",
    "    print(f\"Stage 2 Epoch {epoch_i+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping based on val loss\n",
    "    if val_loss < best_val_loss_stage2:\n",
    "        best_val_loss_stage2 = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_final.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping during Stage 2.\")\n",
    "            break\n",
    "\n",
    "# Load best model from stage 2\n",
    "model.load_state_dict(torch.load('best_model_final.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds, true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids, b_input_mask, b_cat, b_num, b_labels = tuple(t.to(device) for t in batch)\n",
    "        logits = model(b_input_ids, b_input_mask, b_cat, b_num)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        t = b_labels.cpu().numpy()\n",
    "        preds.append(p)\n",
    "        true.append(t)\n",
    "\n",
    "preds = np.concatenate(preds)\n",
    "true = np.concatenate(true)\n",
    "print(\"Final Classification Report (Validation):\")\n",
    "print(classification_report(true, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data\n",
    "test_df = data_df[data_df['Usage'] == 'Test'].copy()\n",
    "test_df['cleaned_title'] = test_df['cleaned_title'].fillna('')\n",
    "test_df['cleaned_description'] = test_df['cleaned_description'].fillna('')\n",
    "test_df['Component'] = test_df['Component'].astype(str)\n",
    "test_df['Status'] = test_df['Status'].astype(str)\n",
    "test_df['Resolution'] = test_df['Resolution'].astype(str)\n",
    "\n",
    "# Combine text\n",
    "test_df['text_input'] = test_df.apply(lambda row: row['cleaned_title'] + \" [SEP] \" + row['cleaned_description'], axis=1)\n",
    "\n",
    "# Tokenize test text\n",
    "test_input_ids, test_attention_masks = [], []\n",
    "for sent in test_df['text_input']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "# Categorical features\n",
    "test_cat_component = test_df['Component'].map(component2idx).values\n",
    "test_cat_status = test_df['Status'].map(status2idx).values\n",
    "test_cat_resolution = test_df['Resolution'].map(resolution2idx).values\n",
    "test_cat_feats = torch.tensor(np.stack([test_cat_component, test_cat_status, test_cat_resolution], axis=1), dtype=torch.long)\n",
    "\n",
    "# Numeric features\n",
    "test_num = test_df[['word_count', 'description_word_count']].fillna(0).values.astype(np.float32)\n",
    "# Normalize with same mean and std from training\n",
    "test_num = (test_num - num_mean) / num_std\n",
    "test_num_feats = torch.tensor(test_num, dtype=torch.float32)\n",
    "\n",
    "# Create Test Dataset and DataLoader\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_cat_feats, test_num_feats)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model_final.pt'))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Predicting on Test\"):\n",
    "        b_input_ids, b_input_mask, b_cat, b_num = tuple(t.to(device) for t in batch)\n",
    "        logits = model(b_input_ids, b_input_mask, b_cat, b_num)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = test_df[['Issue_id']].copy()\n",
    "submission_df['Priority'] = all_preds\n",
    "submission_df.to_csv('submission_roberta.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.28.post3-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: numpy in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from xformers) (1.23.5)\n",
      "Collecting torch==2.5.1 (from xformers)\n",
      "  Using cached torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (2024.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (9.1.0.70)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->xformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (12.4.127)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->xformers)\n",
      "  Using cached triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from torch==2.5.1->xformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from sympy==1.13.1->torch==2.5.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/da23c014/miniconda3/envs/isec/lib/python3.9/site-packages (from jinja2->torch==2.5.1->xformers) (2.1.5)\n",
      "Downloading xformers-0.0.28.post3-cp39-cp39-manylinux_2_28_x86_64.whl (16.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, xformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1\n",
      "    Uninstalling torch-2.4.1:\n",
      "      Successfully uninstalled torch-2.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lightautoml 0.3.8.1 requires torch<=2.0.0,>=1.9.0, but you have torch 2.5.1 which is incompatible.\n",
      "torchaudio 2.4.1+cu124 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\n",
      "torchvision 0.19.1 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 triton-3.1.0 xformers-0.0.28.post3\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2200de112a1a483d80d771722ea737e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffef723ca3b24515847737a970382413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81befc2724d242f69c34132498cbbdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae820b9d1a4c6a807d03f62f7c232a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8946bf7a6df748d399fd45dfe202cb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/186 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60954d32800e433191516e877978f88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense_1024/config.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3917e73713774be5a93169328de2a0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94adaf7871f540aab68b28624550577c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"dunzhang/stella_en_400M_v5\", trust_remote_code=True)\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
